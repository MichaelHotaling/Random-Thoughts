{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "041d9411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import cv2\n",
    "from typing import List\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import chromadb\n",
    "from chromadb.api.segment import API\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from chromadb.config import Settings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f407f84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_SETTINGS = Settings(\n",
    "        persist_directory='db',\n",
    "        anonymized_telemetry=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c568e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.base import BaseLoader, Document\n",
    "from langchain.document_loaders import (\n",
    "    CSVLoader,\n",
    "    EverNoteLoader,\n",
    "    PyMuPDFLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredEmailLoader,\n",
    "    UnstructuredEPubLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredODTLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69fe14e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVideoLoader(BaseLoader):\n",
    "    def __init__(self, file_path, model = 'base'):\n",
    "        import whisper\n",
    "        \n",
    "        self.file_path = file_path\n",
    "        if model.strip().lower() in whisper.available_models():\n",
    "            self.model = whisper.load_model(model)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"{model} is not available. \"\n",
    "                \"Please select one of the following models.\\n\"\n",
    "                f\"{whisper.available_models()}\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def __timestamp__(t):\n",
    "        \n",
    "        hours, minutes = divmod(int(t),3600)\n",
    "        minutes, seconds = divmod(minutes, 60)\n",
    "\n",
    "        hours, minutes, seconds = str(hours).zfill(2), str(minutes).zfill(2), str(seconds).zfill(2)\n",
    "        return f\"{hours}:{minutes}:{seconds}\"\n",
    "\n",
    "    def load(self):\n",
    "        text = \"\"\n",
    "        result = self.model.transcribe(self.file_path)\n",
    "        for line in result['segments']:\n",
    "            text += f\"{self.__timestamp__(line['start'])}-{self.__timestamp__(line['end'])} || {line['text']}\\n\"\n",
    "        metadata = {\"source\": self.file_path}\n",
    "        return [Document(page_content=text, metadata=metadata)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d782af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEmailLoader(UnstructuredEmailLoader):\n",
    "    \"\"\"Wrapper to fallback to text/plain when default does not work\"\"\"\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Wrapper adding fallback for elm without html\"\"\"\n",
    "        try:\n",
    "            try:\n",
    "                doc = UnstructuredEmailLoader.load(self)\n",
    "            except ValueError as e:\n",
    "                if 'text/html content not found in email' in str(e):\n",
    "                    # Try plain text\n",
    "                    self.unstructured_kwargs[\"content_source\"]=\"text/plain\"\n",
    "                    doc = UnstructuredEmailLoader.load(self)\n",
    "                else:\n",
    "                    raise \n",
    "        except Exception as e:\n",
    "            # Add file_path to exception message\n",
    "            raise type(e)(f\"{self.file_path}: {e}\") from e\n",
    "\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6484aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map file extensions to document loaders and their arguments\n",
    "LOADER_MAPPING = {\n",
    "    \".csv\": (CSVLoader, {\"encoding\": \"utf8\"}),\n",
    "    \".doc\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".enex\": (EverNoteLoader, {}),\n",
    "    \".eml\": (CustomEmailLoader, {}),\n",
    "    \".epub\": (UnstructuredEPubLoader, {}),\n",
    "    \".html\": (UnstructuredHTMLLoader, {}),\n",
    "    \".md\": (UnstructuredMarkdownLoader, {}),\n",
    "    \".odt\": (UnstructuredODTLoader, {}),\n",
    "    \".pdf\": (PyMuPDFLoader, {}),\n",
    "    \".ppt\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".pptx\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n",
    "    \".mp4\": (CustomVideoLoader, {\"model\":'base'}),\n",
    "    \".mp3\": (CustomVideoLoader, {\"model\":'base'})\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "785568a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomVideoLoader(r'C:\\Users\\hotal\\Downloads\\OpenAI_DevDay.mp4', model = 'base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9821a204",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = model.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d307bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "060ef941",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_single_document(file_path: str) -> List[Document]:\n",
    "    ext = \".\" + file_path.split(\".\")[-1].lower()\n",
    "    if ext in LOADER_MAPPING:\n",
    "        loader_class, loader_args = LOADER_MAPPING[ext]\n",
    "        loader = loader_class(file_path, **loader_args)\n",
    "        return loader.load()\n",
    "    else:\n",
    "        print(f\"Unsupported file extension: '{ext}'\\n\"\n",
    "              \"Skipping file\")\n",
    "        \n",
    "\n",
    "def load_documents(source_dir: str, ignored_files: List[str] = [], loaded_file_set_file: str = \"loaded_files.pkl\") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Loads all documents from the source documents directory, ignoring specified files and skipping already loaded files\n",
    "    \"\"\"\n",
    "    # Load existing set of loaded files\n",
    "    loaded_files = set()\n",
    "    if os.path.exists(loaded_file_set_file):\n",
    "        with open(loaded_file_set_file, 'rb') as f:\n",
    "            loaded_files = pickle.load(f)\n",
    "\n",
    "    all_files = []\n",
    "    \n",
    "    for foldername, subfolders, filenames in os.walk(source_dir):\n",
    "        for ext in LOADER_MAPPING:\n",
    "            all_files.extend([\n",
    "                os.path.join(foldername, filename)\n",
    "                for filename in filenames\n",
    "                if filename.lower().endswith(ext.lower()) or filename.upper().endswith(ext.upper())\n",
    "            ])\n",
    "    \n",
    "    # Filter out ignored files and already loaded files\n",
    "    filtered_files = [file_path for file_path in all_files if file_path not in ignored_files and file_path not in loaded_files]\n",
    "\n",
    "    with Pool(processes=os.cpu_count()) as pool:\n",
    "        results = []\n",
    "        with tqdm(total=len(filtered_files), desc='Loading new documents', ncols=80) as pbar:\n",
    "            for i, docs in enumerate(pool.imap_unordered(load_single_document, filtered_files)):\n",
    "                results.extend(docs)\n",
    "                # Update the set of loaded files\n",
    "                loaded_files.add(filtered_files[i])\n",
    "                pbar.update()\n",
    "\n",
    "    # Save the updated set of loaded files to pickle file\n",
    "    with open(loaded_file_set_file, 'wb') as f:\n",
    "        pickle.dump(loaded_files, f)\n",
    "\n",
    "    return results\n",
    "\n",
    "def process_documents(ignored_files: List[str] = []) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load documents and split in chunks\n",
    "    \"\"\"\n",
    "    print(f\"Loading documents from {source_directory}\")\n",
    "    documents = load_documents(source_directory, ignored_files)\n",
    "    if not documents:\n",
    "        print(\"No new documents to load\")\n",
    "        exit(0)\n",
    "    print(f\"Loaded {len(documents)} new documents from {source_directory}\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    documents = text_splitter.split_documents(documents)\n",
    "    print(f\"Split into {len(documents)} chunks of text (max. {chunk_size} tokens each)\")\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f31b3788",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cfcfeec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ccc2f66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name = 'all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "015cf6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma(\n",
    "    embedding_function= embeddings, \n",
    "    persist_directory = 'db2'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "de6e093b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['448d17e9-8642-11ee-904b-803049142908',\n",
       " '448d17ea-8642-11ee-82fb-803049142908',\n",
       " '448d17eb-8642-11ee-90b8-803049142908',\n",
       " '448d17ec-8642-11ee-9c23-803049142908',\n",
       " '448d17ed-8642-11ee-a516-803049142908',\n",
       " '448d17ee-8642-11ee-aaed-803049142908',\n",
       " '448d17ef-8642-11ee-8076-803049142908',\n",
       " '448d17f0-8642-11ee-b554-803049142908',\n",
       " '448d17f1-8642-11ee-a5b0-803049142908',\n",
       " '448d17f2-8642-11ee-935f-803049142908',\n",
       " '448d17f3-8642-11ee-aaec-803049142908',\n",
       " '448d17f4-8642-11ee-b18c-803049142908',\n",
       " '448d17f5-8642-11ee-b16b-803049142908',\n",
       " '448d17f6-8642-11ee-bec3-803049142908',\n",
       " '448d17f7-8642-11ee-942c-803049142908',\n",
       " '448d17f8-8642-11ee-99c5-803049142908',\n",
       " '448d17f9-8642-11ee-8bdc-803049142908',\n",
       " '448d17fa-8642-11ee-aed8-803049142908',\n",
       " '448d17fb-8642-11ee-9871-803049142908',\n",
       " '448d17fc-8642-11ee-8384-803049142908',\n",
       " '448d17fd-8642-11ee-95e1-803049142908',\n",
       " '448d17fe-8642-11ee-ab5c-803049142908',\n",
       " '448d17ff-8642-11ee-b9ea-803049142908',\n",
       " '448d1800-8642-11ee-90e8-803049142908',\n",
       " '448d1801-8642-11ee-a4a8-803049142908',\n",
       " '448d1802-8642-11ee-9479-803049142908',\n",
       " '448d1803-8642-11ee-98d7-803049142908',\n",
       " '448d1804-8642-11ee-8114-803049142908',\n",
       " '448d1805-8642-11ee-a6ae-803049142908',\n",
       " '448d1806-8642-11ee-a73d-803049142908',\n",
       " '448d1807-8642-11ee-9da0-803049142908',\n",
       " '448d1808-8642-11ee-94b8-803049142908',\n",
       " '448d1809-8642-11ee-998c-803049142908',\n",
       " '448d85de-8642-11ee-aa5d-803049142908',\n",
       " '448d85df-8642-11ee-890f-803049142908',\n",
       " '448d85e0-8642-11ee-a5b8-803049142908',\n",
       " '448d85e1-8642-11ee-a44e-803049142908',\n",
       " '448d85e2-8642-11ee-8ca6-803049142908',\n",
       " '448d85e3-8642-11ee-9f18-803049142908',\n",
       " '448d85e4-8642-11ee-82fc-803049142908',\n",
       " '448d85e5-8642-11ee-a810-803049142908',\n",
       " '448d85e6-8642-11ee-a68a-803049142908',\n",
       " '448d85e7-8642-11ee-a1c4-803049142908',\n",
       " '448d85e8-8642-11ee-8951-803049142908',\n",
       " '448d85e9-8642-11ee-a852-803049142908',\n",
       " '448d85ea-8642-11ee-b35e-803049142908',\n",
       " '448d85eb-8642-11ee-a20d-803049142908',\n",
       " '448d85ec-8642-11ee-8de9-803049142908',\n",
       " '448d85ed-8642-11ee-9746-803049142908',\n",
       " '448d85ee-8642-11ee-8ec4-803049142908',\n",
       " '448d85ef-8642-11ee-a7f7-803049142908',\n",
       " '448d85f0-8642-11ee-b5ff-803049142908',\n",
       " '448d91bd-8642-11ee-876b-803049142908',\n",
       " '448d91be-8642-11ee-97f5-803049142908',\n",
       " '448d91bf-8642-11ee-92a4-803049142908',\n",
       " '448d91c0-8642-11ee-a38e-803049142908']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.add_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74e0fe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7250792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "952bf335",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma(embedding_function=embeddings, persist_directory='db2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0aa5d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1175bbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4e3ab636",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "364980fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = qa(\"Whats new at OpenAI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1144621d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI has introduced GPTs, custom versions of chat GPT for a specific purpose that combine instructions, extended knowledge and actions. They have also launched the Assistance API to make it easier to build assisted experiences with apps, and a new GPT-4 Turbo model that delivers improved function calling, knowledge, lowered pricing, new modalities and more. They are also deepening their partnership with Microsoft.\n"
     ]
    }
   ],
   "source": [
    "print(res['result'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5bfc3d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hotal\\Downloads\\OpenAI_DevDay.mp4\n",
      "00:20:28-00:20:32 ||  The upsides of this are going to be tremendous.\n",
      "00:20:32-00:20:40 ||  At OpenAI, we really believe that gradual iterative deployment is the best way to address the safety issues, the safety challenges with AI.\n",
      "00:20:40-00:20:44 ||  We think it's especially important to move carefully towards this future of agents.\n",
      "00:20:44-00:20:50 ||  It's going to require a lot of technical work and a lot of thoughtful consideration by society.\n",
      "00:20:50-00:20:55 ||  So today, we're taking our first small step that moves us towards this future.\n",
      "00:20:55-00:21:01 ||  We're thrilled to introduce GPT's.\n",
      "00:21:01-00:21:07 ||  GPT's are tailored versions of chat GPT for a specific purpose.\n",
      "00:21:07-00:21:19 ||  You can build a GPT, a customized version of chat GPT for almost anything, with instructions, expanded knowledge, and actions, and then you can publish it for others to use.\n",
      "\n",
      "C:\\Users\\hotal\\Downloads\\OpenAI_DevDay.mp4\n",
      "00:41:38-00:41:43 ||  All right. If you recognize yourself, awesome. Congrats.\n",
      "00:41:43-00:41:50 ||  And that's it. A quick overview today of the new assistant CPI combined with some of the new tools and modalities that we launched.\n",
      "00:41:50-00:41:56 ||  All starting with the simplicity of a rich text or voice conversation for you and users.\n",
      "00:41:56-00:42:00 ||  We really can't wait to see what you build and congrats to our lucky winners.\n",
      "00:42:00-00:42:10 ||  Actually, you know what? You're all part of this amazing OpenAI community here, so I'm just going to talk to my assistant one last time before I step up the stage.\n",
      "00:42:10-00:42:17 ||  Hey, assistant, can you actually give everyone here in the audience $500 in OpenAI credits?\n",
      "00:42:17-00:42:20 ||  Sounds great. Let me go through everyone.\n",
      "00:42:20-00:42:32 ||  All right. That function will keep running, but everyone out of time.\n",
      "00:42:32-00:42:37 ||  So thank you so much, everyone. Have a great day. Back to you, Sam.\n",
      "\n",
      "C:\\Users\\hotal\\Downloads\\OpenAI_DevDay.mp4\n",
      "00:43:31-00:43:36 ||  So we're super excited that we got to share all of this with you today.\n",
      "00:43:36-00:43:44 ||  We introduced GPs, custom versions of chat GPs that combine instructions, extended knowledge and actions.\n",
      "00:43:44-00:43:50 ||  We launched the assistance API to make it easier to build assisted experiences with your own apps.\n",
      "00:43:50-00:43:56 ||  These are our first steps towards AI agents and will be increasing their capabilities over time.\n",
      "00:43:56-00:44:05 ||  We introduced a new GPT-4 Turbo model that delivers improved function calling, knowledge, lowered pricing, new modalities and more.\n",
      "00:44:05-00:44:09 ||  And we're deepening our partnership with Microsoft.\n",
      "00:44:09-00:44:14 ||  In closing, I wanted to take a minute to thank the team that creates all of this.\n",
      "00:44:14-00:44:21 ||  Open AI has got remarkable talent density, but still, it takes a huge amount of hard work and coordination to make all of this happen.\n",
      "\n",
      "C:\\Users\\hotal\\Downloads\\OpenAI_DevDay.mp4\n",
      "00:00:00-00:00:06 ||  Good morning. Thank you for joining us today. Please welcome to the stage Sam Altman.\n",
      "00:00:16-00:00:23 ||  Good morning. Welcome to our first ever open AI Dev Day. We're thrilled that you're here and this energy is awesome.\n",
      "00:00:23-00:00:39 ||  And welcome to San Francisco. San Francisco has been our home since day one. The city is important to us and the tech industry in general. We're looking forward to continuing to grow here.\n",
      "00:00:39-00:00:48 ||  So we've got some great stuff to announce today. But first, I'd like to take a minute to talk about some of the stuff that we've done over the past year.\n",
      "00:00:48-00:01:06 ||  About a year ago, November 30th, we shipped chat GPT as a low-key research preview. And that went pretty well. In March, we followed that up with the launch of GPT-4. Still, the most capable model out in the world.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in res['source_documents']:\n",
    "    print(i.metadata['source'])\n",
    "    print(i.page_content)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cb265372",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = qa(\"What is the new GPT4 turbo model and what can it do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4328a5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' GPT4 Turbo is a new model that supports up to 128,000 tokens of context, 300 pages of a standard book, and is much more accurate over a long context. It can browse the web, write and run code, analyze data, generate images, and more.'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "71d13bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:12:07-00:12:13 ||  All right, there's actually one more developer request that's been even bigger than all of these.\n",
      "00:12:13-00:12:16 ||  And so I'd like to talk about that now.\n",
      "00:12:16-00:12:21 ||  And that's pricing.\n",
      "00:12:21-00:12:25 ||  So what's going on here is the industry leading model.\n",
      "00:12:25-00:12:32 ||  It delivers a lot of improvements that we just covered and it's a smarter model than GPT 4.\n",
      "00:12:32-00:12:39 ||  We've heard from developers that there are a lot of things that they want to build, but GPT 4 just cost too much.\n",
      "00:12:39-00:12:45 ||  They've told us that if we could decrease the cost by 20, 25% that would be great.\n",
      "00:12:46-00:12:56 ||  I'm super excited to announce that we worked really hard on this and GPT 4 Turbo, a better model is considerably cheaper than GPT 4.\n",
      "00:12:56-00:13:05 ||  By a factor of 3x for prompt tokens.\n",
      "00:13:05-00:13:13 ||  And 2x for completion tokens starting today.\n",
      "\n",
      "00:18:53-00:18:56 ||  See you.\n",
      "00:19:02-00:19:08 ||  Okay. So we have shared a lot of great updates for developers already. And we got a lot more to come.\n",
      "00:19:08-00:19:15 ||  But even though this is developer conference, we can't resist making some improvements to chat GPT.\n",
      "00:19:15-00:19:23 ||  So a small one, chat GPT now uses GPT for turbo with all the latest improvements, including the latest knowledge cutoff, which will continue update.\n",
      "00:19:23-00:19:25 ||  That's all live today.\n",
      "00:19:25-00:19:32 ||  It can now browse the web when it needs to write and run code, analyze data, take in generate images and much more.\n",
      "00:19:32-00:19:36 ||  And we heard your feedback that model picker, extremely annoying, that is gone starting today.\n",
      "00:19:36-00:19:40 ||  You will not have to click around to drop down menu. All of this will just work together.\n",
      "00:19:40-00:19:47 ||  Chat GPT. Yeah.\n",
      "\n",
      "00:05:24-00:05:27 ||  It's really informed that we have to show you today.\n",
      "00:05:27-00:05:30 ||  Today, we are launching a new model.\n",
      "00:05:30-00:05:33 ||  GPT4 Turbo.\n",
      "00:05:33-00:05:42 ||  GPT4 Turbo will address many of the things that you all have asked for.\n",
      "00:05:42-00:05:48 ||  So let's go through what's new. We've got six major things to talk about for this part.\n",
      "00:05:48-00:05:56 ||  Number one, context length. A lot of people have tasks that require a much longer context length.\n",
      "00:05:56-00:06:01 ||  GPT4 supported up to 8K, and in some cases up to 32K context length.\n",
      "00:06:01-00:06:05 ||  But we know that isn't enough for many of you in what you want to do.\n",
      "00:06:05-00:06:09 ||  GPT4 Turbo supports up to 128,000 tokens of context.\n",
      "00:06:15-00:06:20 ||  That's 300 pages of a standard book, 16 times longer than our 8K context.\n",
      "00:06:20-00:06:28 ||  In addition to longer context length, you'll notice that the model is much more accurate over a long context.\n",
      "\n",
      "00:14:05-00:14:12 ||  Running a fine tune GPT 3.5 Turbo 16K version is also cheaper than the old fine tuned 4K version.\n",
      "00:14:12-00:14:16 ||  Okay, so we just covered a lot about the model itself.\n",
      "00:14:16-00:14:19 ||  We hope that these changes address your feedback.\n",
      "00:14:19-00:14:23 ||  We're really excited to bring all of these improvements to everybody now.\n",
      "00:14:23-00:14:29 ||  In all of this, we're lucky to have a partner who is instrumental in making it happen.\n",
      "00:14:29-00:14:34 ||  So I'd like to bring out a special guest, Satin Adela, the CEO of Microsoft.\n",
      "00:14:34-00:14:39 ||  Thank you.\n",
      "00:14:39-00:14:41 ||  Thank you so much.\n",
      "00:14:41-00:14:42 ||  Thank you.\n",
      "00:14:42-00:14:45 ||  Satin, thanks so much for coming here.\n",
      "00:14:45-00:14:48 ||  It's fantastic to be here and Sam congrats.\n",
      "00:14:48-00:14:52 ||  I've been really looking forward to Turbo and everything else that you have coming.\n",
      "00:14:52-00:14:53 ||  It's been just fantastic.\n",
      "00:14:53-00:14:54 ||  Awesome.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in res['source_documents']:\n",
    "    print(i.page_content)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9f1d181b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: Never Gonna Give You Up\n",
      "Resolution: 720p\n",
      "File Size: 19.26 MB\n",
      "Download complete!\n"
     ]
    }
   ],
   "source": [
    "from pytube import YouTube\n",
    "\n",
    "def download_youtube_video(video_url, output_path=\".\"):\n",
    "    try:\n",
    "        # Create a YouTube object\n",
    "        yt = YouTube(video_url)\n",
    "\n",
    "        # Get the highest resolution stream\n",
    "        video_stream = yt.streams.get_highest_resolution()\n",
    "\n",
    "        # Display video details\n",
    "        print(\"Downloading:\", yt.title)\n",
    "        print(\"Resolution:\", video_stream.resolution)\n",
    "        print(\"File Size:\", round(video_stream.filesize / (1024 * 1024), 2), \"MB\")\n",
    "\n",
    "        # Download the video\n",
    "        video_stream.download(output_path, )\n",
    "        print(\"Download complete!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", str(e))\n",
    "\n",
    "# Example usage:\n",
    "video_url = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"\n",
    "output_path = \"\"\n",
    "\n",
    "download_youtube_video(video_url, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "23584d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.88G/2.88G [06:26<00:00, 7.99MiB/s]\n"
     ]
    }
   ],
   "source": [
    "model = CustomVideoLoader(\"Never Gonna Give You Up.mp4\", model='large')\n",
    "data = model.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8786db16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:00-00:00:00 ||  ðŸŽµ\n",
      "00:00:00-00:00:22 ||  ðŸŽµ We're no strangers to love ðŸŽµ\n",
      "00:00:22-00:00:27 ||  ðŸŽµ You know the rules and so do I ðŸŽµ\n",
      "00:00:27-00:00:31 ||  ðŸŽµ I've built commitments while I'm thinking of ðŸŽµ\n",
      "00:00:31-00:00:35 ||  ðŸŽµ You wouldn't get this from any other guy ðŸŽµ\n",
      "00:00:35-00:00:40 ||  ðŸŽµ I just wanna tell you how I'm feeling ðŸŽµ\n",
      "00:00:40-00:00:43 ||  ðŸŽµ Gotta make you understand ðŸŽµ\n",
      "00:00:43-00:00:45 ||  ðŸŽµ Never gonna give you up ðŸŽµ\n",
      "00:00:45-00:00:47 ||  ðŸŽµ Never gonna let you down ðŸŽµ\n",
      "00:00:47-00:00:51 ||  ðŸŽµ Never gonna run around and desert you ðŸŽµ\n",
      "00:00:51-00:00:53 ||  ðŸŽµ Never gonna make you cry ðŸŽµ\n",
      "00:00:53-00:00:55 ||  ðŸŽµ Never gonna say goodbye ðŸŽµ\n",
      "00:00:55-00:00:56 ||  ðŸŽµ Never gonna tell you goodbye ðŸŽµ\n",
      "00:00:57-00:01:00 ||  ðŸŽµ Never gonna lie and hurt you ðŸŽµ\n",
      "00:01:00-00:01:04 ||  ðŸŽµ We've known each other for so long ðŸŽµ\n",
      "00:01:04-00:01:07 ||  ðŸŽµ Your heart's been aching but ðŸŽµ\n",
      "00:01:07-00:01:09 ||  ðŸŽµ You're too shy to say it ðŸŽµ\n",
      "00:01:09-00:01:13 ||  ðŸŽµ Inside we both know what's been going on ðŸŽµ\n",
      "00:01:13-00:01:17 ||  ðŸŽµ We know the game and we're gonna play it ðŸŽµ\n",
      "00:01:17-00:01:22 ||  ðŸŽµ And if you ask me how I'm feeling ðŸŽµ\n",
      "00:01:22-00:01:25 ||  ðŸŽµ Don't tell me you're too blind to see ðŸŽµ\n",
      "00:01:25-00:01:26 ||  ðŸŽµ Never gonna give you up ðŸŽµ\n",
      "00:01:27-00:01:29 ||  ðŸŽµ Never gonna let you down ðŸŽµ\n",
      "00:01:29-00:01:33 ||  ðŸŽµ Never gonna run around and desert you ðŸŽµ\n",
      "00:01:33-00:01:35 ||  ðŸŽµ Never gonna make you cry ðŸŽµ\n",
      "00:01:35-00:01:37 ||  ðŸŽµ Never gonna say goodbye ðŸŽµ\n",
      "00:01:37-00:01:41 ||  ðŸŽµ Never gonna tell you bye and hurt you ðŸŽµ\n",
      "00:01:41-00:01:43 ||  ðŸŽµ Never gonna give you up ðŸŽµ\n",
      "00:01:43-00:01:45 ||  ðŸŽµ Never gonna let you down ðŸŽµ\n",
      "00:01:45-00:01:50 ||  ðŸŽµ Never gonna run around and desert you ðŸŽµ\n",
      "00:01:50-00:01:52 ||  ðŸŽµ Never gonna make you cry ðŸŽµ\n",
      "00:01:52-00:01:54 ||  ðŸŽµ Never gonna say goodbye ðŸŽµ\n",
      "00:01:54-00:01:56 ||  ðŸŽµ Never gonna tell you bye ðŸŽµ\n",
      "00:01:57-00:01:58 ||  And hurt you\n",
      "00:01:58-00:02:03 ||  Give you love\n",
      "00:02:03-00:02:07 ||  Give you love\n",
      "00:02:07-00:02:09 ||  Never gonna give\n",
      "00:02:09-00:02:11 ||  Give you love\n",
      "00:02:11-00:02:13 ||  Never gonna give\n",
      "00:02:13-00:02:15 ||  Give you love\n",
      "00:02:15-00:02:18 ||  We've known each other\n",
      "00:02:18-00:02:20 ||  For so long\n",
      "00:02:20-00:02:22 ||  Your heart's been aching but\n",
      "00:02:22-00:02:25 ||  You're too shy to say it\n",
      "00:02:25-00:02:28 ||  It's sad we both know what's been going on\n",
      "00:02:28-00:02:33 ||  We know the game and we're gonna play it\n",
      "00:02:33-00:02:37 ||  I just wanna tell you how I'm feeling\n",
      "00:02:37-00:02:41 ||  Gotta make you understand\n",
      "00:02:41-00:02:43 ||  Never gonna give you up\n",
      "00:02:43-00:02:45 ||  Never gonna let you down\n",
      "00:02:45-00:02:49 ||  Never gonna run around and desert you\n",
      "00:02:49-00:02:51 ||  Never gonna make you cry\n",
      "00:02:51-00:02:53 ||  Never gonna say goodbye\n",
      "00:02:53-00:02:55 ||  Never gonna tell\n",
      "00:02:55-00:02:55 ||  Never gonna say goodbye\n",
      "00:02:55-00:03:25 ||  Never gonna make you cry\n",
      "00:03:25-00:03:25 ||  Never gonna let you down\n",
      "00:03:25-00:03:27 ||  Never gonna say goodbye\n",
      "00:03:27-00:03:29 ||  Never gonna tell you bye\n",
      "00:03:30-00:03:32 ||  Never gonna say goodbye\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f896730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
